{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import * \n",
    "import html \n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=pathlib.Path('data/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "#! curl -O http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "#! tar -xzfv aclImdb_v1.tar.gz -C {DATA_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = 'xbos' # beginning of sentence tag \n",
    "FLD = 'xfld' # data field tag \n",
    "\n",
    "PATH = pathlib.Path('data/aclImdb/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLAS_PATH=pathlib.Path('data/imdb_clas/')\n",
    "CLAS_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LM_PATH=pathlib.Path('data/imdb_lm/')\n",
    "LM_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "CLASSES = ['neg', 'pos', 'unsup']\n",
    "\n",
    "def get_texts(path):\n",
    "    '''\n",
    "    input : text data with labels in front of sentence\n",
    "    output : array text with associated label\n",
    "    ''' \n",
    "    texts, labels = [], []\n",
    "    for idx, label in enumerate(CLASSES):\n",
    "        for fname in (path/label).glob('*.*'):\n",
    "            texts.append(fname.open('r', encoding='utf-8').read())\n",
    "            labels.append(idx)\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "trn_txts, trn_labels = get_texts(PATH/'train')\n",
    "val_txts, val_labels = get_texts(PATH/'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000, 25000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_txts), len(val_txts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "trn_idx = np.random.permutation(len(trn_txts))\n",
    "val_idx = np.random.permutation(len(val_txts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_texts = trn_txts[trn_idx]\n",
    "val_texts = val_txts[val_idx]\n",
    "\n",
    "trn_labels = trn_labels[trn_idx]\n",
    "val_labels = trn_labels[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "col_names = ['labels', 'text']\n",
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':trn_labels}, columns=col_names) \n",
    "\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':val_labels}, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>I love all of Linda Howard's books, and was th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The Lack of content in this movie amazed me th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>You know what they say about the 70's..if you ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>An old grandfather, Don Plutarco plays the vio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>At first, I actually had no idea that Billy Bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text\n",
       "0       2  I love all of Linda Howard's books, and was th...\n",
       "1       0  The Lack of content in this movie amazed me th...\n",
       "2       1  You know what they say about the 70's..if you ...\n",
       "3       2  An old grandfather, Don Plutarco plays the vio...\n",
       "4       2  At first, I actually had no idea that Billy Bl..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_trn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I'd read all the negative reviews for \"Anna Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dude, I thought this movie rocked. Perfect for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A nurse travels to a rural psychiatric clinic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>An \"independant\" film that, from the back of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>This is just a very bad film. Miles looks as i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   labels                                               text\n",
       "0       1  I'd read all the negative reviews for \"Anna Ch...\n",
       "1       2  Dude, I thought this movie rocked. Perfect for...\n",
       "2       2  A nurse travels to a rural psychiatric clinic ...\n",
       "3       0  An \"independant\" film that, from the back of t...\n",
       "4       2  This is just a very bad film. Miles looks as i..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg\\n', 'pos\\n', 'unsup\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(CLAS_PATH/'classes.txt').open('w').writelines(f'{o}\\n' for o in CLASSES)\n",
    "(CLAS_PATH/'classes.txt').open().readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90000 10000\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "trn_texts, val_texts = sklearn.model_selection.train_test_split(np.concatenate([trn_texts, val_texts]), test_size=0.1)\n",
    "print(len(trn_texts), len(val_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)})\n",
    "df_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trn.to_csv(LM_PATH/'trn.csv', header=False, index=False)\n",
    "df_val.to_csv(LM_PATH/'val.csv', header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "re1 = re.compile(r'  +')\n",
    "def fixup(x):\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['text', 'labels']\n",
    "df_trn = pd.read_csv(LM_PATH/'trn.csv', nrows=10000, names=columns)\n",
    "df_val = pd.read_csv(LM_PATH/'val.csv', nrows=10000, names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(df):\n",
    "    labels = df['labels'].values.astype(np.int64)\n",
    "    texts = f'\\n{BOS} {FLD} 1 ' + df['text'].astype(str)\n",
    "    texts = texts.apply(fixup).values.astype(str)\n",
    "    tok = Tokenizer().process_all(texts)\n",
    "    return tok, list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn, trn_labels = get_text(df_trn)\n",
    "tok_val, val_labels = get_text(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tok_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n xbos xfld 1 ... xxmaj must admit well acted , but \" dark \" & depressing film portraying a wannabe stand - up \" comic \" xxrep 4 . with no clue toward humour . xxmaj viewed this film at xxmaj the xxmaj stony xxmaj brook xxmaj film xxmaj festival . xxmaj one of a \" book - end \" weekend topping off another xxmaj frank xxmaj whaley vehicle ... \" xxmaj the xxmaj pursuit of xxmaj happiness \" . xxmaj frank ( and his brother xxmaj robert , as his xxmaj tops boss xxmaj mr. xxmaj slocum ) & his buddy xxmaj ethan xxmaj hawke stand out in this film as hopeless \" common men \" ... stuck in a rut of xxmaj life \\'s problems and of no seeing any chance of rising above it . xxmaj frank ( xxmaj jimmy o\\'brien ) sees a calling as a stand up comic , but a mix of stage fright and overwhelming domestic problems , put him in a trance , undermining a \" true \" escape from his downward spiraling xxmaj life . xxmaj see it for the acting ( which is top notch ) , but as with earlier comments , if u need a feel good and less tragic movie see xxmaj frank xxmaj whaley in \" xxmaj the xxmaj pursuit of xxmaj happiness \" .'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tok_trn[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LM_PATH/'tmp').mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'tok_trn.npy', tok_trn)\n",
    "np.save(LM_PATH/'tmp'/'tok_val.npy', tok_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_trn = np.load(LM_PATH/'tmp'/'tok_trn.npy')\n",
    "tok_val = np.load(LM_PATH/'tmp'/'tok_val.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxmaj', 251392),\n",
       " ('the', 133084),\n",
       " ('.', 109877),\n",
       " (',', 108332),\n",
       " ('a', 64756),\n",
       " ('and', 64641),\n",
       " ('of', 57861),\n",
       " ('to', 53727),\n",
       " ('is', 43355),\n",
       " ('it', 38237),\n",
       " ('in', 37177),\n",
       " ('i', 34361),\n",
       " ('this', 29773),\n",
       " ('that', 28716),\n",
       " ('\"', 25836),\n",
       " (\"'s\", 24325),\n",
       " ('-', 21109),\n",
       " ('was', 20159),\n",
       " ('\\n\\n', 20000),\n",
       " ('as', 18345),\n",
       " ('with', 17632),\n",
       " ('for', 17456),\n",
       " ('movie', 17393),\n",
       " ('xxup', 17337),\n",
       " ('but', 16629)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = Counter(p for o in tok_trn for p in o)\n",
    "freq.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vocab = 60000\n",
    "min_freq = 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = [o for o, c in freq.most_common(max_vocab) if c > min_freq]\n",
    "itos.insert(0, '_pad_')\n",
    "itos.insert(0, '_unk_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25883"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import collections\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k, v in enumerate(itos)})\n",
    "len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "pickle.dump(itos, open(LM_PATH/'tmp'/'itos.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', trn_lm)\n",
    "np.save(LM_PATH/'tmp'/'trn_ids.npy', val_lm)\n",
    "itos = pickle.load(open(LM_PATH/'tmp'/'itos.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25883, 10000)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs=len(itos)\n",
    "vs, len(trn_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-12-12 10:51:50--  http://files.fast.ai/models/wt103/\n",
      "Resolving files.fast.ai (files.fast.ai)... 67.205.15.147\n",
      "Connecting to files.fast.ai (files.fast.ai)|67.205.15.147|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:51:51 (31.4 MB/s) - `data/aclImdb/models/wt103/index.html' saved [857/857]\n",
      "\n",
      "Loading robots.txt; please ignore errors.\n",
      "--2018-12-12 10:51:51--  http://files.fast.ai/robots.txt\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2018-12-12 10:51:52 ERROR 404: Not Found.\n",
      "\n",
      "--2018-12-12 10:51:52--  http://files.fast.ai/models/wt103/?C=N;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html?C=N;O=D'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:51:52 (62.9 MB/s) - `data/aclImdb/models/wt103/index.html?C=N;O=D' saved [857/857]\n",
      "\n",
      "--2018-12-12 10:51:52--  http://files.fast.ai/models/wt103/?C=M;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html?C=M;O=A'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:51:52 (34.1 MB/s) - `data/aclImdb/models/wt103/index.html?C=M;O=A' saved [857/857]\n",
      "\n",
      "--2018-12-12 10:51:52--  http://files.fast.ai/models/wt103/?C=S;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html?C=S;O=A'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:51:52 (58.4 MB/s) - `data/aclImdb/models/wt103/index.html?C=S;O=A' saved [857/857]\n",
      "\n",
      "--2018-12-12 10:51:52--  http://files.fast.ai/models/wt103/?C=D;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html?C=D;O=A'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:51:53 (43.0 MB/s) - `data/aclImdb/models/wt103/index.html?C=D;O=A' saved [857/857]\n",
      "\n",
      "--2018-12-12 10:51:53--  http://files.fast.ai/models/wt103/bwd_wt103.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387687 (441M) [text/plain]\n",
      "Saving to: `data/aclImdb/models/wt103/bwd_wt103.h5'\n",
      "\n",
      "models/wt103/bwd_wt 100%[===================>] 440.97M  5.01MB/s    in 97s     \n",
      "\n",
      "2018-12-12 10:53:31 (4.52 MB/s) - `data/aclImdb/models/wt103/bwd_wt103.h5' saved [462387687/462387687]\n",
      "\n",
      "--2018-12-12 10:53:31--  http://files.fast.ai/models/wt103/bwd_wt103_enc.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387634 (441M) [text/plain]\n",
      "Saving to: `data/aclImdb/models/wt103/bwd_wt103_enc.h5'\n",
      "\n",
      "models/wt103/bwd_wt 100%[===================>] 440.97M  7.56MB/s    in 77s     \n",
      "\n",
      "2018-12-12 10:54:48 (5.75 MB/s) - `data/aclImdb/models/wt103/bwd_wt103_enc.h5' saved [462387634/462387634]\n",
      "\n",
      "--2018-12-12 10:54:48--  http://files.fast.ai/models/wt103/fwd_wt103.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387687 (441M) [text/plain]\n",
      "Saving to: `data/aclImdb/models/wt103/fwd_wt103.h5'\n",
      "\n",
      "models/wt103/fwd_wt 100%[===================>] 440.97M  15.8MB/s    in 34s     \n",
      "\n",
      "2018-12-12 10:55:22 (12.9 MB/s) - `data/aclImdb/models/wt103/fwd_wt103.h5' saved [462387687/462387687]\n",
      "\n",
      "--2018-12-12 10:55:22--  http://files.fast.ai/models/wt103/fwd_wt103_enc.h5\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 462387634 (441M) [text/plain]\n",
      "Saving to: `data/aclImdb/models/wt103/fwd_wt103_enc.h5'\n",
      "\n",
      "models/wt103/fwd_wt 100%[===================>] 440.97M  17.0MB/s    in 26s     \n",
      "\n",
      "2018-12-12 10:55:48 (17.0 MB/s) - `data/aclImdb/models/wt103/fwd_wt103_enc.h5' saved [462387634/462387634]\n",
      "\n",
      "--2018-12-12 10:55:48--  http://files.fast.ai/models/wt103/itos_wt103.pkl\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4161252 (4.0M) [text/plain]\n",
      "Saving to: `data/aclImdb/models/wt103/itos_wt103.pkl'\n",
      "\n",
      "models/wt103/itos_w 100%[===================>]   3.97M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2018-12-12 10:55:49 (39.4 MB/s) - `data/aclImdb/models/wt103/itos_wt103.pkl' saved [4161252/4161252]\n",
      "\n",
      "--2018-12-12 10:55:49--  http://files.fast.ai/models/wt103/?C=N;O=A\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html?C=N;O=A'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:55:49 (54.5 MB/s) - `data/aclImdb/models/wt103/index.html?C=N;O=A' saved [857/857]\n",
      "\n",
      "--2018-12-12 10:55:49--  http://files.fast.ai/models/wt103/?C=M;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html?C=M;O=D'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:55:50 (58.4 MB/s) - `data/aclImdb/models/wt103/index.html?C=M;O=D' saved [857/857]\n",
      "\n",
      "--2018-12-12 10:55:50--  http://files.fast.ai/models/wt103/?C=S;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html?C=S;O=D'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:55:50 (54.5 MB/s) - `data/aclImdb/models/wt103/index.html?C=S;O=D' saved [857/857]\n",
      "\n",
      "--2018-12-12 10:55:50--  http://files.fast.ai/models/wt103/?C=D;O=D\n",
      "Reusing existing connection to files.fast.ai:80.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 857 [text/html]\n",
      "Saving to: `data/aclImdb/models/wt103/index.html?C=D;O=D'\n",
      "\n",
      "models/wt103/index. 100%[===================>]     857  --.-KB/s    in 0s      \n",
      "\n",
      "2018-12-12 10:55:50 (62.9 MB/s) - `data/aclImdb/models/wt103/index.html?C=D;O=D' saved [857/857]\n",
      "\n",
      "FINISHED --2018-12-12 10:55:50--\n",
      "Total wall clock time: 4m 0s\n",
      "Downloaded: 14 files, 1.7G in 3m 54s (7.54 MB/s)\n"
     ]
    }
   ],
   "source": [
    "! wget -nH -r -np -P {PATH} http://files.fast.ai/models/wt103/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. pre-trained lm has embedding size, hidden units and layers that we need to match to our IMDB language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz, nh, nl = 400, 1150, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPATH = PATH/'models'/'wt103'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_LM_PATH = PREPATH/'fwd_wt103.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. get the mean encoder weight and apply the mean to tokens present in IMDB dataset but not present in lm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc:storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.018296, -0.138256,  0.014381, -0.012851, ...,  0.003654,  0.004884,  0.057428, -0.007599], dtype=float32)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts = np.array(wgts['0.encoder.weight'])\n",
    "row_m = enc_wgts.mean(0)\n",
    "row_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "itos2 = pickle.load((PRE_PATH/'itos_wt103.pkl').open('rb'))\n",
    "stoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400,)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_w = np.zeros((vs, em_sz))\n",
    "new_w[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.485352e-05, -2.342431e-05,  1.969303e-05, -2.154383e-05, ...,  2.051086e-05,  2.134880e-05,  2.177563e-05,\n",
       "       -1.239415e-05], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_wgts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 25883 is out of bounds for axis 0 with size 25883",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-98b63b1f1e33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mem_sz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstoi2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mnew_w\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_wgts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 25883 is out of bounds for axis 0 with size 25883"
     ]
    }
   ],
   "source": [
    "new_w = np.zeros((vs, em_sz), dtype=np.float32)\n",
    "for i, w in enumerate(stoi2):\n",
    "    new_w[i] = enc_wgts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Users/nus/.fastai/data/imdb_sample/texts.csv')]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "list(path.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextLMDataBunch.from_csv(path, 'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = TextClasDataBunch.from_csv(path, 'texts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='5%'>  <col width='95%'>  <tr>\n",
       "    <th>idx</th>\n",
       "    <th>text</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>0</th>\n",
       "    <th>xxbos xxfld 1 xxmaj to be honest i had heard this was pretty bad before i decided to watch it , but i 'm never one to let others influence my viewings , in fact i 'm more likely to watch something out of xxunk xxmaj xxunk had one thing going for me before the viewing anyway , the fact that xxmaj xxunk xxmaj xxunk and those gorgeous eyes was</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>; though , it 's possible i 've seen him in a less memorable role . xxmaj haines makes an incredible impression , when he xxunk xxmaj davies for a xxunk meal - xxunk his hat into the ring with some wonderful bits at the xxunk table . xxmaj indeed , xxmaj haines and xxmaj davies deliver great comic performances . \\n\\n xxmaj the story starts off with xxmaj xxunk</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>2</th>\n",
       "    <th>love of classic poetry , the sea , a tall tale , that almost rings true , and a story that has left a lasting impact on our world and culture . xxmaj who does not understand the meaning of an \" xxunk \" ? or the concept of \" water , water everywhere and not a drop to drink ? \" a truly fine experience . xxmaj thank you</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>3</th>\n",
       "    <th>! ) \\n\\n xxmaj the movie explores the absurdity of the situation . xxmaj the thinking that bars women from football xxunk comes down to it being too xxunk an experience for the xxunk xxunk philosophy not unknown in the west less than 100 years ago . xxmaj this farce comes to a head when a girl needs to go to the bathroom , so a soldier xxunk her demands</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>4</th>\n",
       "    <th>xxmaj mendes has delivered a fine follow - up to his xxmaj oscar - winning debut , a film which is as intelligent as it is beautiful to watch . \" xxmaj road xxmaj to xxmaj perdition \" may not be to everyone 's tastes but this is one xxup dvd i shall not be xxunk anytime soon . xxbos xxfld 1 xxmaj fair drama / love story movie that</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>  <col width='90%'>  <col width='10%'>  <tr>\n",
       "    <th>text</th>\n",
       "    <th>target</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos xxfld 1 xxup the xxup shop xxup around xxup the xxup corner is one of the xxunk and most feel - good romantic comedies ever made . xxmaj there 's just no getting around that , and it 's hard to actually put one 's feeling for this film into words . xxmaj it 's not one of those films that tries too hard , nor does it come</th>\n",
       "    <th>positive</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos xxfld 1 xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" xxmaj at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n\\n xxmaj it 's usually satisfying to watch a film director change his style /</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos xxfld 1 xxmaj many neglect that this is n't just a classic due to the fact that it 's the first xxup 3d game , or even the first xxunk - up . xxmaj it 's also one of the first xxunk games , one of the xxunk definitely the first ) truly claustrophobic games , and just a pretty well - xxunk xxunk experience in general . xxmaj</th>\n",
       "    <th>positive</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos xxfld 1 i really wanted to love this show . i truly , honestly did . \\n\\n xxmaj for the first time , gay viewers get their own version of the \" xxmaj the xxmaj bachelor \" . xxmaj with the help of his obligatory \" hag \" xxmaj xxunk , xxmaj james , a good looking , well - to - do thirty - something has the chance</th>\n",
       "    <th>negative</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>xxbos xxfld 1 \\n\\n i 'm sure things did n't exactly go the same way in the real life of xxmaj homer xxmaj hickam as they did in the film adaptation of his book , xxmaj rocket xxmaj boys , but the movie \" xxmaj october xxmaj sky \" ( an xxunk of the book 's title ) is good enough to stand alone . i have not read xxmaj</th>\n",
       "    <th>positive</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data_lm.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos</td>\n",
       "      <td>she</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>ca</td>\n",
       "      <td>.</td>\n",
       "      <td>,</td>\n",
       "      <td>the</td>\n",
       "      <td>probably</td>\n",
       "      <td>that</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxfld</td>\n",
       "      <td>has</td>\n",
       "      <td>some</td>\n",
       "      <td>nt</td>\n",
       "      <td>i</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>country</td>\n",
       "      <td>the</td>\n",
       "      <td>almost</td>\n",
       "      <td>an</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>of</td>\n",
       "      <td>blame</td>\n",
       "      <td>regret</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>(</td>\n",
       "      <td>most</td>\n",
       "      <td>cut</td>\n",
       "      <td>ideal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxmaj</td>\n",
       "      <td>same</td>\n",
       "      <td>the</td>\n",
       "      <td>them</td>\n",
       "      <td>that</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>annoying</td>\n",
       "      <td>it</td>\n",
       "      <td>introduction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jack</td>\n",
       "      <td>stupid</td>\n",
       "      <td>best</td>\n",
       "      <td>since</td>\n",
       "      <td>imdb</td>\n",
       "      <td>frost</td>\n",
       "      <td>connecticut</td>\n",
       "      <td>character</td>\n",
       "      <td>in</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxmaj</td>\n",
       "      <td>grin</td>\n",
       "      <td>movies</td>\n",
       "      <td>the</td>\n",
       "      <td>can</td>\n",
       "      <td>,</td>\n",
       "      <td>)</td>\n",
       "      <td>to</td>\n",
       "      <td>half</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>frost</td>\n",
       "      <td>in</td>\n",
       "      <td>that</td>\n",
       "      <td>events</td>\n",
       "      <td>only</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>and</td>\n",
       "      <td>\"</td>\n",
       "      <td>and</td>\n",
       "      <td>corporate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>is</td>\n",
       "      <td>her</td>\n",
       "      <td>are</td>\n",
       "      <td>are</td>\n",
       "      <td>allow</td>\n",
       "      <td>nick</td>\n",
       "      <td>soon</td>\n",
       "      <td>grace</td>\n",
       "      <td>crippled</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxmaj</td>\n",
       "      <td>face</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>all</td>\n",
       "      <td>a</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>find</td>\n",
       "      <td>\"</td>\n",
       "      <td>him</td>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>really</td>\n",
       "      <td>.</td>\n",
       "      <td>as</td>\n",
       "      <td>wildly</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>an</td>\n",
       "      <td>the</td>\n",
       "      <td>but</td>\n",
       "      <td>sexual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>a</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>\"</td>\n",
       "      <td>unimaginative</td>\n",
       "      <td>of</td>\n",
       "      <td>and</td>\n",
       "      <td>old</td>\n",
       "      <td>screen</td>\n",
       "      <td>later</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>xxmaj</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>comedies</td>\n",
       "      <td>and</td>\n",
       "      <td>one</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>ruin</td>\n",
       "      <td>in</td>\n",
       "      <td>he</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cool</td>\n",
       "      <td>it</td>\n",
       "      <td>\"</td>\n",
       "      <td>lame</td>\n",
       "      <td>star</td>\n",
       "      <td>joseph</td>\n",
       "      <td>that</td>\n",
       "      <td>recent</td>\n",
       "      <td>miraculously</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xxmaj</td>\n",
       "      <td>is</td>\n",
       "      <td>actually</td>\n",
       "      <td>.</td>\n",
       "      <td>rating</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>both</td>\n",
       "      <td>times</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>movie</td>\n",
       "      <td>not</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>\\n\\n</td>\n",
       "      <td>and</td>\n",
       "      <td>fiennes</td>\n",
       "      <td>imagine</td>\n",
       "      <td>.</td>\n",
       "      <td>,</td>\n",
       "      <td>by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>only</td>\n",
       "      <td>between</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>not</td>\n",
       "      <td></td>\n",
       "      <td>can</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>after</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>i</td>\n",
       "      <td>the</td>\n",
       "      <td>comedy</td>\n",
       "      <td>finally</td>\n",
       "      <td>zero</td>\n",
       "      <td>but</td>\n",
       "      <td>be</td>\n",
       "      <td>his</td>\n",
       "      <td>getting</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>xxmaj</td>\n",
       "      <td>movie</td>\n",
       "      <td>and</td>\n",
       "      <td>,</td>\n",
       "      <td>or</td>\n",
       "      <td>now</td>\n",
       "      <td>xxunk</td>\n",
       "      <td>voice</td>\n",
       "      <td>arrested</td>\n",
       "      <td>xxunk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>mean</td>\n",
       "      <td>,</td>\n",
       "      <td>drama</td>\n",
       "      <td>the</td>\n",
       "      <td>even</td>\n",
       "      <td>merely</td>\n",
       "      <td>up</td>\n",
       "      <td>/</td>\n",
       "      <td>for</td>\n",
       "      <td>professor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>xxrep</td>\n",
       "      <td>which</td>\n",
       "      <td>.</td>\n",
       "      <td>physical</td>\n",
       "      <td>in</td>\n",
       "      <td>a</td>\n",
       "      <td>as</td>\n",
       "      <td>accent</td>\n",
       "      <td>a</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1         2              3       4        5            6  \\\n",
       "0    xxbos     she     xxmaj             ca       .        ,          the   \n",
       "1    xxfld     has      some             nt       i    xxmaj      country   \n",
       "2        1     the        of          blame  regret    xxunk            (   \n",
       "3    xxmaj    same       the           them    that    xxmaj        xxmaj   \n",
       "4     jack  stupid      best          since    imdb    frost  connecticut   \n",
       "5    xxmaj    grin    movies            the     can        ,            )   \n",
       "6    frost      in      that         events    only    xxmaj          and   \n",
       "7       is     her       are            are   allow     nick         soon   \n",
       "8    xxmaj    face     xxunk            all       a    xxmaj         find   \n",
       "9   really       .        as         wildly   xxunk    xxunk           an   \n",
       "10       a    \\n\\n         \"  unimaginative      of      and          old   \n",
       "11   xxmaj   xxmaj  comedies            and     one    xxmaj         ruin   \n",
       "12    cool      it         \"           lame    star   joseph         that   \n",
       "13   xxmaj      is  actually              .  rating    xxmaj         both   \n",
       "14   movie     not     xxunk           \\n\\n     and  fiennes      imagine   \n",
       "15       .    only   between          xxmaj     not                  can   \n",
       "16       i     the    comedy        finally    zero      but           be   \n",
       "17   xxmaj   movie       and              ,      or      now        xxunk   \n",
       "18    mean       ,     drama            the    even   merely           up   \n",
       "19   xxrep   which         .       physical      in        a           as   \n",
       "\n",
       "            7             8             9  \n",
       "0    probably          that          make  \n",
       "1         the        almost            an  \n",
       "2        most           cut         ideal  \n",
       "3    annoying            it  introduction  \n",
       "4   character            in            to  \n",
       "5          to          half             a  \n",
       "6           \"           and     corporate  \n",
       "7       grace      crippled         xxunk  \n",
       "8           \"           him            on  \n",
       "9         the           but        sexual  \n",
       "10     screen         later         xxunk  \n",
       "11         in            he             .  \n",
       "12     recent  miraculously         xxmaj  \n",
       "13      times         xxunk         xxunk  \n",
       "14          .             ,            by  \n",
       "15      xxmaj         after             a  \n",
       "16        his       getting         xxmaj  \n",
       "17      voice      arrested         xxunk  \n",
       "18          /           for     professor  \n",
       "19     accent             a             ,  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = x[:20, :10].cpu()\n",
    "texts = pd.DataFrame([data_lm.train_ds.vocab.textify(l).split(' ') for l in example])\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 04:19 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>5.044475</th>\n",
       "    <th>4.403967</th>\n",
       "    <th>0.238250</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = language_model_learner(data_lm, pretrained_model=URLs.WT103, drop_mult=0.5)\n",
    "learn.fit_one_cycle(1, 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 07:56 <p><table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>accuracy</th>\n",
       "  </tr>\n",
       "  <tr>\n",
       "    <th>1</th>\n",
       "    <th>4.309422</th>\n",
       "    <th>3.920209</th>\n",
       "    <th>0.288031</th>\n",
       "  </tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(1, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
